{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers -qq","metadata":{"execution":{"iopub.status.busy":"2024-01-03T03:20:19.394647Z","iopub.execute_input":"2024-01-03T03:20:19.394938Z","iopub.status.idle":"2024-01-03T03:20:46.731665Z","shell.execute_reply.started":"2024-01-03T03:20:19.394912Z","shell.execute_reply":"2024-01-03T03:20:46.730448Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Load Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nfinetuned_model_checkpoint = \"iamsubrata/Llama-2-7b-chat-hf-sharded-bf16-fine-tuned\"\n\nbitsandbytes_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(finetuned_model_checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(finetuned_model_checkpoint, quantization_config=bitsandbytes_config, device_map={\"\":0})","metadata":{"execution":{"iopub.status.busy":"2024-01-03T03:23:07.357021Z","iopub.execute_input":"2024-01-03T03:23:07.357396Z","iopub.status.idle":"2024-01-03T03:24:38.788095Z","shell.execute_reply.started":"2024-01-03T03:23:07.357364Z","shell.execute_reply":"2024-01-03T03:24:38.787192Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/719 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb3522d3d5d4212858050d4a8cff729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"578525effcb648c3b5f4970c93421444"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffc25fc7ddf740c5972138bdc3a4bef5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e6bbdacabf4be0b003c422413ee1e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73277084ecca4283a52136406990c4a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"382ac9779e534743a660147eae3d4113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"575f2c35802844d9bd79fa624da00005"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00003.bin:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dc5915cabe34340b7b238b7c5d67780"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00003.bin:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa40ee978d354767a8067467f9ad0d4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00003.bin:   0%|          | 0.00/3.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a2b1f3cf2b0463ead5ce26bf549e7d1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be21fe4f8098403f9c1dcd0397c0c1e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/192 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f476a9faa9438c82f065870d2c46ca"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* **iamsubrata/Llama-2-7b-chat-hf-sharded-bf16-fine-tuned**: FineTuned **Llama-2-7b-chat-hf-sharded-bf16** (optimized for dialogue use cases) \non `Enhglish Quotes Dataset`.\n\n* **BitsAndBytesConfig** is used to configure for **model quantization**, which is a technique to reduce the memory and computation requirements of LLM by using lower-precision data types.\n    * **load_in_4bit**: Load the model weights in 4-bit precision, which reduces the model size by **75%** compared to **16-bit precision**.\n    \n    * **bnb_4bit_use_double_quant**: Leverages **double quantization**, which is a method to further compress the model by using `two quantization levels`: one for the **most frequent values** and one **for the rest**.\n    \n    * **bnb_4bit_quant_type**: Quantization type: **nf4**, which stands for **nibble float 4**, a custom data type that uses 4 bits to\n    represent floating-point numbers.\n    \n    - **bnb_4bit_compute_dtype**: This specifies the data type to use for the model computation, **torch.bfloat16**, which is a `16-bit floating-point` format that preserves more dynamic range than 16-bit integer formats.\n\n* A **causal language model** is a type of generative model that **predicts the next token in a sequence given the previous tokens**.","metadata":{}},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"* **TextStreamer** class enables streaming of the generated text, which means that the text is printed to the standard output as soon as each token is generated, instead of waiting for the whole response to be generated.","metadata":{}},{"cell_type":"code","source":"from transformers import TextStreamer\n\n# Define a stream *without* function calling capabilities\ndef stream(user_prompt,model):\n    runtimeFlag = \"cuda:0\"\n    system_prompt = 'You are a helpful assistant that provides accurate and concise responses'\n\n    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n\n    prompt = f\"{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\\n\\n\"\n\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n\n    streamer = TextStreamer(tokenizer)\n\n    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T03:25:37.975624Z","iopub.execute_input":"2024-01-03T03:25:37.976634Z","iopub.status.idle":"2024-01-03T03:25:37.984601Z","shell.execute_reply.started":"2024-01-03T03:25:37.976599Z","shell.execute_reply":"2024-01-03T03:25:37.983611Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"stream('Can you tell me quotes related to universe?.',model)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T04:11:40.898387Z","iopub.execute_input":"2024-01-03T04:11:40.899192Z","iopub.status.idle":"2024-01-03T04:12:09.393403Z","shell.execute_reply.started":"2024-01-03T04:11:40.899157Z","shell.execute_reply":"2024-01-03T04:12:09.392664Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<s> [INST] <<SYS>>\nYou are a helpful assistant that provides accurate and concise responses\n<</SYS>>\n\nCan you tell me quotes related to universe?. [/INST]\n\nOf course! Here are some quotes related to the universe:\n\n1. \"The universe is a pretty big place. If it's just us, seems like an awful waste of space.\" - Carl Sagan\n2. \"The universe is not only stranger than we think, it is stranger than we can think.\" - Werner Heisenberg\n3. \"The universe is not a fixed, unchanging thing, but a constantly evolving, dynamic system.\" - Brian Greene\n4. \"The universe is a vast and intricate tapestry, woven from threads of matter, energy, and information.\" - Michio Kaku\n5. \"The universe is not only a vast and complex system, but it is also a system of relationships, interdependencies, and interactions.\" - Stephen Hawking\n6. \"The universe is a mirror of the human soul, reflecting our deepest desires, fears, and hopes.\" - Unknown\n7. \"The universe is a vast and mysterious place, full of wonders and secrets waiting to be uncovered.\" - Neil deGrasse Tyson\n8. \"The universe is a reflection of our own minds, a manifestation of our deepest desires and fears.\" - Terence McKenna\n9. \"The universe is a place of endless discovery, a never-ending journey of exploration and wonder.\" - Unknown\n10. \"The universe is a vast and beautiful tapestry, woven from the threads of time, space, and matter.\" - Unknown\nI hope you find these quotes inspiring and thought-provoking! Is there anything else I can help you with?</s>\n","output_type":"stream"}]},{"cell_type":"code","source":"stream('Make me Laugh.',model)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T04:12:16.047655Z","iopub.execute_input":"2024-01-03T04:12:16.048018Z","iopub.status.idle":"2024-01-03T04:12:19.267744Z","shell.execute_reply.started":"2024-01-03T04:12:16.047987Z","shell.execute_reply":"2024-01-03T04:12:19.266947Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"<s> [INST] <<SYS>>\nYou are a helpful assistant that provides accurate and concise responses\n<</SYS>>\n\nMake me Laugh. [/INST]\n\nWhy did the scarecrow win an award?\n\nBecause he was outstanding in his field! ðŸ˜‚ðŸŒ½</s>\n","output_type":"stream"}]},{"cell_type":"code","source":"stream('Tell me some thoughtful quotes with deep meaning.',model)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T04:12:34.698185Z","iopub.execute_input":"2024-01-03T04:12:34.698920Z","iopub.status.idle":"2024-01-03T04:13:09.346941Z","shell.execute_reply.started":"2024-01-03T04:12:34.698885Z","shell.execute_reply":"2024-01-03T04:13:09.346194Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"<s> [INST] <<SYS>>\nYou are a helpful assistant that provides accurate and concise responses\n<</SYS>>\n\nTell me some thoughtful quotes with deep meaning. [/INST]\n\nCertainly! Here are some thoughtful quotes with deep meaning that you might find inspiring or helpful:\n\n1. \"The future belongs to those who believe in the beauty of their dreams.\" - Eleanor Roosevelt\n\nThis quote emphasizes the importance of having faith in one's aspirations and dreams. It suggests that those who believe in their dreams are more likely to create a brighter future for themselves and those around them.\n\n2. \"The greatest glory in living lies not in never falling, but in rising every time we fall.\" - Nelson Mandela\n\nThis quote highlights the importance of resilience and perseverance in the face of adversity. It encourages us to embrace challenges and setbacks as opportunities for growth and learning, rather than giving up or losing hope.\n\n3. \"The biggest adventure you can take is to live the life of your dreams.\" - Oprah Winfrey\n\nThis quote emphasizes the importance of pursuing one's passions and living a life that is true to oneself. It encourages us to take risks and pursue our dreams, rather than settling for a life that is less than fulfilling.\n\n4. \"The future is not something we enter, but something we create.\" - Unknown\n\nThis quote emphasizes the power of individual agency and creativity in shaping our future. It suggests that we have the ability to create the future we want, rather than simply being passive recipients of events.\n\n5. \"The best way to predict the future is to create it.\" - Peter Drucker\n\nThis quote highlights the importance of proactivity and creativity in shaping our future. It suggests that by taking action and creating the future we want, we are more likely to achieve our goals and create a brighter future for ourselves and those around us.\n\nI hope you find these quotes inspiring and helpful! Is there anything else I can help you with?</s>\n","output_type":"stream"}]}]}